import Header from "./Header"
import Footer from "./Footer"
function Session() {

    return (
        <>
          <Header/>
            <div id="et-main-area">
                <div id="main-content">
                    <article id="post-2356" className="post-2356 page type-page status-publish hentry">
                        <div className="entry-content">
                            <div className="at-above-post-page addthis_tool" data-url="https://www.ds3-datascience-polytechnique.fr/practical-sessions/"></div><div id="et-boc" className="et-boc">

                                <div className="et-l et-l--post">
                                    <div className="et_builder_inner_content et_pb_gutters3">
                                        <div className="et_pb_section et_pb_section_0 et_section_regular" >

                                        <div className="et_pb_row et_pb_row_0">
                                            <div className="et_pb_column et_pb_column_4_4 et_pb_column_0  et_pb_css_mix_blend_mode_passthrough et-last-child">


                                                <div className="et_pb_module et_pb_text et_pb_text_0  et_pb_text_align_left et_pb_bg_layout_light">


                                                    <div className="et_pb_text_inner"><h3 className="p1" style={{textAlign: "center"}}>In-depth tutorials with practical sessions will take place on January 4th, 5th, and 9th</h3>
                                                        <p style={{textAlign: "center"}}>10 to 12 participants per session: registrations after application acceptance.<br />The sessions will be 3 hours long.</p>
                                                        <p style={{textAlign: "center"}}>Below is the list of confirmed sessions as of today (click on the session title to see detailed information).</p></div>
                                                </div>
                                            </div>


                                        </div> 
                                        <div className="et_pb_row et_pb_row_1">
                                            <div className="et_pb_column et_pb_column_4_4 et_pb_column_1  et_pb_css_mix_blend_mode_passthrough et-last-child">
                                                <div className="et_pb_module et_pb_toggle et_pb_toggle_0 et_pb_toggle_item  et_pb_toggle_close">
                                                    <h5 className="et_pb_toggle_title">ALGORITHMS, THEORY, AND APPLICATIONS OF SUBMODULAR OPTIMIZATION: FROM DISCRETE TO CONTINUOUS AND BACK</h5>
                                                    <div className="et_pb_toggle_content clearfix"><p><b>Organizer</b>: <a href="https://www.ds3-datascience-polytechnique.fr/speakers/">Hamed HASSANI</a></p>
                                                        <p>&nbsp;</p>
                                                        <p style={{textAlign: "justify"}}><strong style={{fontSize: "14px"}}>Abstract</strong><span style={{fontSize: "14px"}}>:<br />This tutorial will cover recent advancements in discrete optimization methods prevalent in large-scale machine learning problems. Traditionally, machine learning has been harnessing convex optimization to design fast algorithms with provable guarantees for a broad range of applications. In recent years, however, there has been a surge of interest in applications that involve discrete optimization. For discrete domains, the analog of convexity is considered to be submodularity, and the evolving theory of submodular optimization has been a catalyst for progress in extraordinarily varied application areas including active learning and experimental design, vision, sparse reconstruction, graph inference, video analysis, clustering, document summarization, object detection, information retrieval, network inference, interpreting neural network, and discrete adversarial attacks.</span></p>
                                                        <p style={{textAlign: "justify"}}>As applications and techniques of submodular optimization mature, a fundamental gap between theory and application emerges. In the past decade, paradigms such as large-scale learning, distributed systems, and sequential decision making have enabled a quantum leap in the performance of learning methodologies. Incorporating these paradigms in discrete problems has led to fundamentally new frameworks for submodular optimization. The goal of this tutorial is to cover rigorous and scalable foundations for discrete optimization in complex, dynamic environments, addressing challenges of scalability and uncertainty, and facilitating distributed and sequential learning in broader discrete settings.</p></div>
                                                </div> 
                                            </div>


                                        </div>


                                    </div> 
                                </div>
                                </div>


                            </div>
                            <div className="at-below-post-page addthis_tool" data-url="https://www.ds3-datascience-polytechnique.fr/practical-sessions/"></div>					</div>


                    </article>



                </div>


                <span className="et_pb_scroll_top et-pb-icon"></span>
            </div>
            <Footer/>
        </>
    )

}
export default Session